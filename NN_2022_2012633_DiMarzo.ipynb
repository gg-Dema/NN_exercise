{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wAEgygyPfO7b",
        "Rl9dZ3HambSz"
      ],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdnnRxTNdyW1"
      },
      "source": [
        "# Neural Networks\n",
        "## Homework 1: Implementing advanced activation functions\n",
        "\n",
        "**Name**: Gabriele G. Di Marzo\n",
        "\n",
        "**Matricola**: 2012633\n",
        "\n",
        "Upload the completed notebook **before 30/11/2022 at 23:59** on the Google Classrom page."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEr8qV6-nMuL"
      },
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAEgygyPfO7b"
      },
      "source": [
        "### Objective\n",
        "\n",
        "The purpose of this homework is to implement a new layer inside PyTorch, by properly extending the `nn.Module` object. **Before proceeding**, carefully read the following documentation:\n",
        "\n",
        "+ [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=module#torch.nn.Module)\n",
        "+ [PyTorch: Custom Module](https://pytorch.org/tutorials/beginner/examples_nn/polynomial_module.html)\n",
        "\n",
        "You can also (optionally) learn more about activation functions by reading the survey in [1]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYYk6NnFXqYr"
      },
      "source": [
        "### Introduction: families of activation functions\n",
        "\n",
        "In the course up to now, we have seen the application of the sigmoid, the softmax, and the ReLU. However, many additional activation functions exist [1], with varying strengths and drawbacks.\n",
        "\n",
        "Several of them are designed as variants of the ReLU. The **S-Shaped ReLU** (SReLU) [2] is defined as:\n",
        "\n",
        "$$\n",
        "\\phi(s) = \\begin{cases} t^r + a^r(s - t^r) & \\text{ if } s > t^r \\\\ s & \\text{ if } t^r > s > t^l \\\\ t^l + a^l(s - t^l) & \\text{ if } s < t^l \\end{cases} \\,.\n",
        "$$\n",
        "\n",
        "The four parameters $t^r, a^r, t^l, a^l$ are trained via back-propagation, and they are **different for each unit in the layer**. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u4aF6Z4maHd"
      },
      "source": [
        "### Exercise 1: implementing an activation function (1 point)\n",
        "\n",
        "Let us start with the simpler **[exponential linear squashing](https://paperswithcode.com/method/elish)** (ELiSH) activation function:\n",
        "\n",
        "$$\n",
        "\\phi(x) = \\begin{cases} \\sigma(x)x & \\text{ if } x \\ge 0 \\\\ \\frac{\\exp(x) - 1}{1 + \\exp(-x)} & \\text{ otherwise} \\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKdIjBuZgc5o"
      },
      "source": [
        "**Exercise 1**: complete the following stub."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfCpFg2xdug_"
      },
      "source": [
        "def elish(x):\n",
        "  # x is a generic torch.Tensor, and this function must compute the ELiSH activation function.\n",
        "    return torch.where(x>=0, torch.sigmoid(x)*x, torch.sigmoid(x)*(torch.exp(x) - 1 ))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8MEZxqbn8vs"
      },
      "source": [
        "**Hints for a correct implementation**:\n",
        "\n",
        "1. There are several ways of implementing an if/else operation like the one above in PyTorch. In general, the simplest implementation of \"*if a then b, else c*\" is `torch.where(a, b, c)` (see the documentation for [torch.where](https://pytorch.org/docs/stable/generated/torch.where.html)). Any working variant is accepted here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9a6dBsb_I11"
      },
      "source": [
        "Here is a simple sanity check for the correct implementation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2rkMj9r_UJG"
      },
      "source": [
        "elish(torch.FloatTensor([[0.2, -0.4]])) # Should be approximately [[0.11, -0.13]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv1-249GftP2"
      },
      "source": [
        "### Exercise 2: some visualization experiments (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWY76mEepSgj"
      },
      "source": [
        "**Exercise 2.1**: plot the ELiSH function in [-5, +5]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdK0CyscfDtC"
      },
      "source": [
        "x = torch.arange(-5, 5, 0.1, requires_grad=True )\n",
        "y = elish(x)\n",
        "plt.plot(x.detach().numpy(), y.detach().numpy())\n",
        "plt.xlim([-5, 5])\n",
        "plt.ylim(y.detach().numpy().min(), x.detach().numpy().max())\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnvkoRVAqwg1"
      },
      "source": [
        "**Exercise 2.2**: using the utilities from `torch.autograd` ([torch.autograd](https://pytorch.org/docs/stable/autograd.html)), **compute and plot** the derivative of ELiSH using automatic differentiation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward(x)"
      ],
      "metadata": {
        "id": "rp9mXGG6yF61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QJKAtmWf3ZI"
      },
      "source": [
        "# TODO: plot the gradient of the ELiSH function\n",
        "plt.grid(True)\n",
        "plt.plot(x.detach().numpy(), x.grad.detach().numpy(), label=\"derivate of elish funct\")\n",
        "plt.xlim([-5, 5])\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yqnx7jRxf2qy"
      },
      "source": [
        "**Exercise 2.3 (sanity check)**: build a model using the previously defined activation function, and test it on a random mini-batch of data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YLbAbBYfJ7D"
      },
      "source": [
        "# TODO: complete the definition of the model\n",
        "class RandomModel(torch.nn.Module): \n",
        "  def __init__(self, funct): \n",
        "    super().__init__()\n",
        "    self.l1 = torch.nn.Linear(10, 10)\n",
        "    self.l2 = torch.nn.Linear(10, 2)\n",
        "    self.funct = funct\n",
        "  \n",
        "  def forward(self, x): \n",
        "    x = self.l1(x)\n",
        "    x = self.funct(x)\n",
        "    x = self.l2(x)\n",
        "    return x\n",
        "    \n",
        "\n",
        "model = RandomModel(elish)\n",
        "\n",
        "print(model(torch.randn((5, 10))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKMV_HkD23K_"
      },
      "source": [
        "## Exercise 3: implementing a trainable activation function (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3PgdzQu3p0L"
      },
      "source": [
        "**Exercise 3:** define a `torch.nn.Module` implementing the SReLU.\n",
        "\n",
        "**Hints for a correct implementation**:\n",
        "* The layer should **only** implement the activation function. Ideally, it will always be used in combination with a fully-connected layer with no activation function.\n",
        "* Think carefully about how you want to initialize the parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2cvBI9N32SA"
      },
      "source": [
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "class SReLU(torch.nn.Module):\n",
        "  def __init__(self, units):\n",
        "    super().__init__()\n",
        "    self.units = units\n",
        "\n",
        "\n",
        "    self.alpha1 = Parameter(torch.randn((self.units), requires_grad=True))\n",
        "    self.beta1 = Parameter(torch.randn((self.units), requires_grad=True))\n",
        "\n",
        "    self.alpha2 = Parameter(torch.randn((self.units), requires_grad=True))\n",
        "    self.beta2 = Parameter(torch.randn((self.units), requires_grad=True))\n",
        "\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return torch.where(x<self.beta1, self.beta1 + self.alpha1*(x - self.beta1),\n",
        "                torch.where(\n",
        "                    self.beta2<=x, x, self.beta2 + self.alpha2*(x-self.beta2) )\n",
        "                 )\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ajdzaIa4-Mt"
      },
      "source": [
        "As a sanity check, initialize a SReLU layer and count the number of parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnATD7hW49sK"
      },
      "source": [
        "# Initialize the layer\n",
        "layer = SReLU(2)\n",
        "# Count the parameters\n",
        "numb_params = sum([p.numel() for p in layer.parameters() if p.requires_grad])\n",
        "p_tot = [p for p in layer.parameters()]\n",
        "print( numb_params ) # Should print 8!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW_Lny-j5h6A"
      },
      "source": [
        "## Exercise 4: training a model with trainable activation functions (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC9Ip1qz5-fp"
      },
      "source": [
        "We will use the following dataset from TensorFlow Datasets:\n",
        "https://www.tensorflow.org/datasets/catalog/german_credit_numeric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdoRIsX66Bww"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import torch.optim as optim \n",
        "from collections import OrderedDict\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,X,Y):\n",
        "    self.X = X                           \n",
        "    self.Y = Y       \n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)                  \n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return [self.X[idx], self.Y[idx]]"
      ],
      "metadata": {
        "id": "yclY7yodB-on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train_data = tfds.load('german_credit_numeric', split='train[:75%]', as_supervised=True)\n",
        "Xtrain, ytrain = train_data.batch(5000).get_single_element()\n"
      ],
      "metadata": {
        "id": "bYlP5pimbMH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = tfds.load('german_credit_numeric', split='train[:75%]', as_supervised=True)\n",
        "Xtrain, ytrain = train_data.batch(5000).get_single_element()\n"
      ],
      "metadata": {
        "id": "DSVqP-52bYdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = tfds.load('german_credit_numeric', split='train[:75%]', as_supervised=True)\n",
        "test_data = tfds.load('german_credit_numeric', split='train[75%:]', as_supervised=True)\n",
        "\n",
        "Xtrain, ytrain = train_data.batch(5000).get_single_element()\n",
        "Xtrain, ytrain = preprocessing.normalize(Xtrain.numpy()), ytrain.numpy()\n",
        "Xtrain, ytrain = torch.Tensor(Xtrain), torch.Tensor(ytrain)\n",
        "\n",
        "Xtest, ytest = test_data.batch(5000).get_single_element()\n",
        "Xtest, ytest = preprocessing.normalize(Xtest.numpy()), ytest.numpy()\n",
        "Xtest, ytest = torch.Tensor(Xtest), torch.Tensor(ytest)"
      ],
      "metadata": {
        "id": "JH2HxGZRceFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset(Xtrain, ytrain), batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset(Xtest, ytest), batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "Rg78pcyTCakD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK6RdPDQ6K7l"
      },
      "source": [
        "**Exercise 4**: write a `nn.Module` using the previous `SReLU`, and train it on the german_credit_numeric dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKnj9-AV6f38"
      },
      "source": [
        "'''\n",
        "class basicModel(torch.nn.Module): \n",
        "  \n",
        "  def __init__(self):\n",
        "    super('basicModel', self).__init__()\n",
        "    self.lin1 = torch.nn.Linear(24, 10)\n",
        "    self.lin2 = torch.nn.Linear(10, 10)\n",
        "    self.lin3 = torch.nn.Linear(10, 5)\n",
        "    self.out = torch.nn.Linear(5, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.nn.LeakyReLU(self.lin1(x))\n",
        "    x = torch.nn.LeakyReLU(self.lin2(x))\n",
        "    x = torch.nn.LeakyReLU(self.lin3(x))\n",
        "    return torch.nn.Sigmoid(self.out(x))\n",
        "'''\n",
        "\n",
        "model = torch.nn.Sequential(OrderedDict([\n",
        "          \n",
        "          ('Linear1', torch.nn.Linear(24,10)),\n",
        "          #('relu', torch.nn.ReLU()),\n",
        "          ('SReLU1', SReLU(10)),\n",
        "          \n",
        "          ('Lin2', torch.nn.Linear(10, 10)),\n",
        "          #('relu2', torch.nn.ReLU()),\n",
        "          ('SReLU2', SReLU(10)),\n",
        "          ('drop2', torch.nn.Dropout(p=0.5)),\n",
        "          \n",
        "          ('Linear3', torch.nn.Linear(10, 5)), \n",
        "          #('relu3', torch.nn.ReLU()),\n",
        "          ('SReLU3', SReLU(5)),\n",
        "\n",
        "          ('Linear4', torch.nn.Linear(5,1)),\n",
        "          ('Sigmoid', torch.nn.Sigmoid())\n",
        "        ]))\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "correct = 0\n",
        "epochs = 325\n",
        "\n",
        "model.train()\n",
        "for epoch in range(epochs): \n",
        "  total_loss = 0.0\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "\n",
        "    inputs, labels = data\n",
        "    y_pred = model(inputs)\n",
        "    loss = criterion(y_pred, labels.unsqueeze(1))\n",
        "\n",
        "    correct += (y_pred == labels).float().sum()\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  losses.append(loss.detach().item())\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "UtdJVYnBEPSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(epochs), losses, 'orange')\n",
        "plt.title('loss funct')\n",
        "plt.show()\n",
        "print(f'ultima loss : {losses[-1]}')\n"
      ],
      "metadata": {
        "id": "7TziqM6WYYXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total = 0 \n",
        "correct = 0\n",
        "acc_test = []\n",
        "acc_train = []\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(test_loader):\n",
        "      inputs, labels = data\n",
        "      outputs = model(inputs)\n",
        "      outputs = [1 if outputs[i]>=0.5 else 0 for i in range(len(outputs))]\n",
        "      correct += sum([1 if outputs[i]==labels[i] else 0 for i in range(len(labels))])\n",
        "    acc_test = correct\n",
        "    correct = 0\n",
        "    for i, data in enumerate(train_loader):\n",
        "      inputs, labels = data\n",
        "      outputs = model(inputs)\n",
        "      outputs = [1 if outputs[i]>=0.5 else 0 for i in range(len(outputs))]\n",
        "      correct += sum([1 if outputs[i]==labels[i] else 0 for i in range(len(labels))])\n",
        "    acc_train = correct\n",
        "\n",
        "print(f'test accuracy   : {acc_test*100/len(test_data)}')\n",
        "print(f'train accuracy  : {acc_train*100/len(train_data)}')\n"
      ],
      "metadata": {
        "id": "GFMSd36rYsQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EJFqN1rOXmML"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_b9dhJ06gjN"
      },
      "source": [
        "**Optionally**, you can plot the distribution (histogram) of the parameters after training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMBJAWFe6vOu"
      },
      "source": [
        "# TODO: plot the histogram\n",
        "plt.hist( ... )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX48ZUcI9mFa"
      },
      "source": [
        "## Final checklist\n",
        "1. Carefully check all code. Insert comments when needed. Search for \"TODO\" to see if you forgot something.\n",
        "2. Run everything one final time. *Please do not send us notebooks with errors or cells that are not working.*\n",
        "3. Upload the completed notebook **before 30/11/2021 23:59** on the Google Classrom page."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl9dZ3HambSz"
      },
      "source": [
        "### References\n",
        "\n",
        "[1] Apicella, A., Donnarumma, F., Isgr√≤, F. and Prevete, R., 2021. [A survey on modern trainable activation functions](https://arxiv.org/abs/2005.00817).\n",
        "\n",
        "[2] Jin, X., Xu, C., Feng, J., Wei, Y., Xiong, J. and Yan, S., 2016. [Deep learning with s-shaped rectified linear activation units](https://arxiv.org/abs/1512.07030). In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 30, No. 1)."
      ]
    }
  ]
}